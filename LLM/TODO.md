# TODO

- [ ] **[gpt4free](https://github.com/xtekky/gpt4free)**
  - [ ] [OpenAI要求GPT4Free下架 否则将起诉](https://zhidx.com/news/37365.html)
  - [x] [MosaicML推出大模型新服务 价格远低于OpenAI](https://zhidx.com/news/37333.html)
  - [x] [谷歌泄密文件：开源AI将击败谷歌和OpenAI](https://zhidx.com/news/37397.html)
    - [ ] [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither?utm_source=post-email-title&publication_id=329241&post_id=119223672&isFreemail=true&utm_medium=email)

- [ ] [ALiBi](https://arxiv.org/pdf/2108.12409.pdf)
  - [ ] [Train Short, Test Long: Attention with Linear Biases (ALiBi) Enables Input Length Extrapolation](https://github.com/ofirpress/attention_with_linear_biases)

- [ ] MosaicML
  - [ ] [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - [ ] [MosaicML StreamingDataset: Fast, Accurate Streaming of Training Data from Cloud Storage](https://www.mosaicml.com/blog/mosaicml-streamingdataset)
  - [ ] [MosaicML Inference: Secure, Private, and Affordable Deployment for Large Models](https://www.mosaicml.com/blog/inference-launch)
  - [ ] [Mosaic LLMs (Part 2): GPT-3 quality for <$500k](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)
  - [ ] [MPT Training Benchmarks](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train/benchmarking)
  - [ ] [mosaicml/llm-foundry](https://github.com/mosaicml/llm-foundry)


- [ ] Transformer Engine
  - [ ] [Using FP8 with Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)
  - [ ] [Transformer Engine](https://github.com/NVIDIA/TransformerEngine)


- [ ] [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents)
- [ ] 
