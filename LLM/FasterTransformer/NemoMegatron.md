
## [NeMo-Megatron-Launcher](https://github.com/NVIDIA/NeMo-Megatron-Launcher#21-gpt-3-models)

### [Overview](https://github.com/NVIDIA/NeMo-Megatron-Launcher#1-model-overview)
* This deep learning (DL) software stack is optimized for DGX SuperPOD configurations using NVIDIA InfiniBand technology to provide efficient on-premises compute for training and inferring complex workloads.
* Our latest techniques, sequence parallelism and selective activation recomputation, bring up to ~30% faster training time for GPT-3 models ranging from 20B to 1T parameters.
* 



## [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)


